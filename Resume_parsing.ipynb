{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resume_parsing.ipynb",
      "provenance": [],
      "mount_file_id": "1F1HBlvvnLkiTWM4F2MDTNxKWDGJxxJbj",
      "authorship_tag": "ABX9TyP+AfNPJW5riw7YGncellEI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vicks-2019/NLP/blob/master/Resume_parsing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLXdJfM9ZaGO",
        "outputId": "82626719-3160-4889-a11e-d1a8cb734f5b"
      },
      "source": [
        "pip install pdfminer.six "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdfminer.six\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/f3/4fec7dabe8802ebec46141345bf714cd1fc7d93cb74ddde917e4b6d97d88/pdfminer.six-20201018-py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 4.1MB/s \n",
            "\u001b[?25hCollecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/de/7054df0620b5411ba45480f0261e1fb66a53f3db31b28e3aa52c026e72d9/cryptography-3.3.1-cp36-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (2.3.0)\n",
            "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.6/dist-packages (from cryptography->pdfminer.six) (1.14.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.20)\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-3.3.1 pdfminer.six-20201018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk80POmAZhwz",
        "outputId": "22ff65ed-7906-4b50-bc34-26e83667d07d"
      },
      "source": [
        "pip install doc2text"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting doc2text\n",
            "  Downloading https://files.pythonhosted.org/packages/1a/8c/79f2abf15af2f90b38fa78558470ed7f566e29a362ee2329a6266cae3dc7/doc2text-0.2.4.tar.gz\n",
            "Collecting PyPDF2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from doc2text) (1.18.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from doc2text) (7.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from doc2text) (1.4.1)\n",
            "Collecting pytesseract\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/e6/a4e9fc8a93c1318540e8de6d8d4beb5749b7960388a7c7f27799fc2dd016/pytesseract-0.3.7.tar.gz\n",
            "Collecting mime\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/94/d211d0a0e1ad71d7b42dc85531f9aa463393bcbee71b9492b2881361406e/mime-0.1.0.tar.gz\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from mime->doc2text) (0.16.0)\n",
            "Building wheels for collected packages: doc2text, PyPDF2, pytesseract, mime\n",
            "  Building wheel for doc2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for doc2text: filename=doc2text-0.2.4-py2.py3-none-any.whl size=8406 sha256=c5850e06ee46a09c46db09bb25f8d1010fb8253eb2dadb8f672408a079fd11bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/bd/4f/09770c50a893079ef668ff4d54b38ee5ed41b8af7381781f87\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-cp36-none-any.whl size=61087 sha256=c2fe7c378a89a61a53ed653a631d92d957b881b21cbf29f4b3ebece2b3608f64\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/84/19/35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.7-py2.py3-none-any.whl size=13943 sha256=34619b9d03be472a9baef4f656b658d09baedc30c4d901a55472fe070c645c84\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/20/7e/1dd0daad1575d5260916bb1e9781246430647adaef4b3ca3b3\n",
            "  Building wheel for mime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mime: filename=mime-0.1.0-cp36-none-any.whl size=39042 sha256=0d17d3b88383931a07eb5d5fa2b5fa3bb34b9ea1c5b8d2a35f865ced89c433b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/fa/95/b3b92f350c8df57becba1faad2ef598cfbeb56188570e6ab8e\n",
            "Successfully built doc2text PyPDF2 pytesseract mime\n",
            "Installing collected packages: PyPDF2, pytesseract, mime, doc2text\n",
            "Successfully installed PyPDF2-1.26.0 doc2text-0.2.4 mime-0.1.0 pytesseract-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkvuglSxjTcB",
        "outputId": "04afc2db-0240-4a1c-b03d-696e2867eb27"
      },
      "source": [
        "pip install docx2txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting docx2txt\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/7d/60ee3f2b16d9bfdfa72e8599470a2c1a5b759cb113c6fe1006be28359327/docx2txt-0.8.tar.gz\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-cp36-none-any.whl size=3963 sha256=c33651b90b020cd9a22747013816d72b4d0da2bf0c00261529d03bacf3a19d7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/1f/26/a051209bbb77fc6bcfae2bb7e01fa0ff941b82292ab084d596\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guNC4EmZZq0T"
      },
      "source": [
        "from pdfminer.converter import TextConverter\r\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\r\n",
        "from pdfminer.pdfinterp import PDFResourceManager\r\n",
        "from pdfminer.layout import LAParams\r\n",
        "from pdfminer.pdfpage import PDFPage\r\n",
        "import io\r\n",
        "\r\n",
        "def extract_text_from_pdf(pdf_path):\r\n",
        "    with open(pdf_path, 'rb') as fh:\r\n",
        "        # iterate over all pages of PDF document\r\n",
        "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\r\n",
        "            # creating a resoure manager\r\n",
        "            resource_manager = PDFResourceManager()\r\n",
        "            \r\n",
        "            # create a file handle\r\n",
        "            fake_file_handle = io.StringIO()\r\n",
        "            \r\n",
        "            # creating a text converter object\r\n",
        "            converter = TextConverter(\r\n",
        "                                resource_manager, \r\n",
        "                                fake_file_handle, \r\n",
        "                                codec='utf-8', \r\n",
        "                                laparams=LAParams()\r\n",
        "                        )\r\n",
        "\r\n",
        "            # creating a page interpreter\r\n",
        "            page_interpreter = PDFPageInterpreter(\r\n",
        "                                resource_manager, \r\n",
        "                                converter\r\n",
        "                            )\r\n",
        "\r\n",
        "            # process current page\r\n",
        "            page_interpreter.process_page(page)\r\n",
        "            \r\n",
        "            # extract text\r\n",
        "            text = fake_file_handle.getvalue()\r\n",
        "            yield text\r\n",
        "\r\n",
        "            # close open handles\r\n",
        "            converter.close()\r\n",
        "            fake_file_handle.close()\r\n",
        "\r\n",
        "# calling above function and extracting text\r\n",
        "text = ''\r\n",
        "for page in extract_text_from_pdf('/content/drive/MyDrive/Resume_NKR (1).pdf'):\r\n",
        "    text += ' ' + page\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWNkNkb3c-9l"
      },
      "source": [
        "import docx2txt\r\n",
        "\r\n",
        "def extract_text_from_doc(doc_path):\r\n",
        "    temp = docx2txt.process(\"resumes/Chinmaya_Kaundanya_Resume.docx\")\r\n",
        "    text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\r\n",
        "    return ' '.join(text)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW5rLFPfjt_w"
      },
      "source": [
        "import spacy\r\n",
        "from spacy.matcher import Matcher\r\n",
        "\r\n",
        "# load pre-trained model\r\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQxfpmnhnOxJ",
        "outputId": "1b0cbc99-c5ae-4ea9-a138-b507e33da7df"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('words')\r\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txzJgfwOp72s",
        "outputId": "290b2fc5-03dd-4889-a26f-b373311eea4b"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ifPnXdrq79Q"
      },
      "source": [
        "import spacy\r\n",
        "from spacy.matcher import Matcher\r\n",
        "\r\n",
        "# load pre-trained model\r\n",
        "nlp = spacy.load('en_core_web_sm')\r\n",
        "\r\n",
        "# initialize matcher with a vocab\r\n",
        "matcher = Matcher(nlp.vocab)\r\n",
        "\r\n",
        "def extract_name(text):\r\n",
        "    nlp_text = nlp(text)\r\n",
        "    \r\n",
        "    # First name and Last name are always Proper Nouns\r\n",
        "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\r\n",
        "    \r\n",
        "    matcher.add('NAME', None, [pattern])\r\n",
        "    \r\n",
        "    matches = matcher(nlp_text)\r\n",
        "    \r\n",
        "    for match_id, start, end in matches:\r\n",
        "        span = nlp_text[start:end]\r\n",
        "        return span.text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YmUxtAKrEQr"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "def extract_mobile_number(text):\r\n",
        "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\r\n",
        "    \r\n",
        "    if phone:\r\n",
        "        number = ''.join(phone[0])\r\n",
        "        if len(number) > 10:\r\n",
        "            return '+' + number\r\n",
        "        else:\r\n",
        "            return number"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeychdvinNza",
        "outputId": "c0aa9cfa-9d07-4d87-fae4-de60b788d0bb"
      },
      "source": [
        "name2 = extract_mobile_number(text)\r\n",
        "print(name2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9205083231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1KajD58rJqM"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "def extract_email(email):\r\n",
        "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\r\n",
        "    if email:\r\n",
        "        try:\r\n",
        "            return email[0].split()[0].strip(';')\r\n",
        "        except IndexError:\r\n",
        "            return None\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgwL05XunEuE",
        "outputId": "f6ef151c-4fcb-4ece-d475-0b063a780094"
      },
      "source": [
        "name1 = extract_email(text)\r\n",
        "print(name1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vickynicky.99@gmail.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjLOBCXonHyK"
      },
      "source": [
        "import re\r\n",
        "import spacy\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "# load pre-trained model\r\n",
        "nlp = spacy.load('en_core_web_sm')\r\n",
        "\r\n",
        "# Grad all general stop words\r\n",
        "STOPWORDS = set(stopwords.words('english'))\r\n",
        "\r\n",
        "# Education Degrees\r\n",
        "EDUCATION = [\r\n",
        "            'BE','B.E.', 'B.E', 'BS', 'B.S', \r\n",
        "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \r\n",
        "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \r\n",
        "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\r\n",
        "        ]\r\n",
        "\r\n",
        "def extract_education(resume_text):\r\n",
        "    nlp_text = nlp(resume_text)\r\n",
        "\r\n",
        "    # Sentence Tokenizer\r\n",
        "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\r\n",
        "\r\n",
        "    edu = {}\r\n",
        "    # Extract education degree\r\n",
        "    for index, text in enumerate(nlp_text):\r\n",
        "        for tex in text.split():\r\n",
        "            # Replace all special symbols\r\n",
        "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\r\n",
        "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\r\n",
        "                edu[tex] = text + nlp_text[index + 1]\r\n",
        "\r\n",
        "    # Extract year\r\n",
        "    education = []\r\n",
        "    for key in edu.keys():\r\n",
        "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\r\n",
        "        if year:\r\n",
        "            education.append((key, ''.join(year[0])))\r\n",
        "        else:\r\n",
        "            education.append(key)\r\n",
        "    return education"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU8haj1bkl0n",
        "outputId": "503fec88-e10d-477a-9420-b456d56c5be0"
      },
      "source": [
        "name = extract_education(text)\r\n",
        "print(name)\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['MS', 'BE']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYxo1EZSwPKY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}